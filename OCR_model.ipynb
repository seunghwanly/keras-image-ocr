{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# use csv file\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "fetch from train_image\n",
    "\n",
    "change dir\n",
    "\n",
    "> images and labels >> characters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "labels = []\n",
    "characters = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./image/199304470.jpg\n",
      "ASCOUGH   YJP\n"
     ]
    }
   ],
   "source": [
    "with open('./data_color_text.csv',newline='') as csvfile:\n",
    "    _reader = csv.DictReader(csvfile)\n",
    "    for row in _reader:\n",
    "        image_path = row['path'] + \".jpg\"\n",
    "        images.append(image_path)\n",
    "        labels.append(row['text_front'])\n",
    "        \n",
    "print(images[0])\n",
    "print(labels[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of images found :  23918\n",
      "number of labels found :  23918\n",
      "number of unique characters :  395\n",
      "Characters present {'산', '온', 'β', '깅', '렙', '한', '핀', '티', '사', '필', 'c', '기', 'y', '후', '생', 'w', 'z', '놀', '+', '타', 'A', 'l', '샤', '명', 'i', '월', '피', '아', 'C', '템', '퓨', '클', '얄', '간', '킥', '제', '취', '설', '체', '업', '胡', '─', '지', '덱', '○', '엔', 'γ', '톤', '엠', '슈', '닥', '멘', '로', '/', '앤', '웰', '킴', 'n', 'o', 'α', '드', 'O', '은', '속', '카', '써', '일', 'd', '윈', 'u', '당', 'F', '투', '교', '졸', '센', 'P', '=', '복', '↑', 'X', '록', '세', '덴', '옥', '컨', '∩', '테', '탄', '풍', '리', '총', '여', '넥', '쿨', '듀', '망', '膠', '징', 'r', '분', '모', '셀', '쓸', '콜', '디', '포', 'g', '버', '균', '치', '액', '.', '|', '?', '컴', '쎌', '경', '나', '겐', '먼', '팩', '큐', '목', '네', '게', '품', '말', '워', 'Q', '남', '터', '림', 'B', 'ル', '∪', '훼', '론', '와', 'T', 'E', '텍', '골', '멕', '턴', '노', 'リ', '니', '뉴', 'j', '므', 'k', '루', 'v', '브', '씨', '억', '락', '든', '½', 'S', '날', '살', 'ㅇ', '다', 'K', '비', 'D', '크', '\\u3000', '▽', ' ', '下', '팍', '더', 'チ', '페', '우', '콤', '딜', '퀸', ',', '벨', '즈', '광', '잇', '캐', '익', '쎄', '탑', '4', '9', 'x', '칼', '그', '캄', '푸', '알', '슘', '단', 'e', '열', '민', '팡', '약', '®', 'W', 'm', '난', '확', '심', '젠', '플', '튼', '心', '펜', '도', '폴', '왕', '화', '섹', '絡', '生', '키', '방', '고', '-', '헤', '스', '쓰', '신', 'R', '連', '빈', '닐', '자', '헵', '柴', '존', '올', '르', '△', 't', 'H', '맥', '1', 'U', '무', '#', '굿', '유', '트', '애', '2', '판', '엘', '안', '너', '웃', '칸', '블', '토', 'Y', '렌', '↔', '딘', '☆', '파', '탐', 'Λ', '가', '러', '대', '동', '8', '정', 'b', '담', '소', '0', '합', '탈', '란', '몬', '해', '오', '콘', '™', '녹', '퀵', '멜', '향', '레', '프', '시', '쿡', '·', '5', '막', '마', '서', 'p', '부', '십', 'I', '에', 'G', '롤', '6', '친', 'ⓢ', '젤', '조', '인', '라', '通', '텐', '논', '휴', '빅', '랠', '펀', '닌', '람', '램', '미', 'マ', '샷', '슬', 'h', '펙', '벤', 'M', '베', '헬', '볼', '메', '본', '쎈', '렉', '할', '코', '실', 'f', '톱', '이', '솔', '허', '근', '옵', 's', '린', '틴', '어', 'Z', '종', '국', '틸', '囊', 'L', '퍼', '원', '？', '작', '바', '7', '―', '씬', '웅', '&', '˙', '하', '3', '감', '★', '임', '누', 'N', '케', '보', '롬', '글', '커', '黃', '현', '엑', '닉', '진', '선', 'J', '♡', '데', '위', 'V', 'a'}\n"
     ]
    }
   ],
   "source": [
    "# Path to the data directory\n",
    "data_dir = Path('./image/')\n",
    "\n",
    "# Get list of all the images\n",
    "# images = sorted(list(map(str, list(data_dir.glob(\"*.jpg\")))))\n",
    "# labels = [img.split(os.path.sep)[-1].split(\".jpg\")[0] for img in images]\n",
    "characters = set(char for label in labels for char in label)\n",
    "\n",
    "print(\"number of images found : \", len(images))\n",
    "print(\"number of labels found : \", len(labels))\n",
    "print(\"number of unique characters : \", len(characters))\n",
    "print(\"Characters present\", characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44\n"
     ]
    }
   ],
   "source": [
    "# batch size for training and validation\n",
    "batch_size = 32\n",
    "# desired image dimensions\n",
    "img_width = 780\n",
    "img_height = 426\n",
    "\n",
    "# Factor by which the image is going to be downsampled\n",
    "# by the convolutional blocks. We will be using two\n",
    "# convolution blocks and each block will have\n",
    "# a pooling layer which downsample the features by a factor of 2.\n",
    "# Hence total downsampling factor would be 4.\n",
    "downsample_factor = 4\n",
    "\n",
    "# Maximum length of any captcha in the dataset\n",
    "max_length = max([len(label) for label in labels])\n",
    "print(max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "get data preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping characters to integers\n",
    "char_to_num = layers.experimental.preprocessing.StringLookup(\n",
    "    vocabulary=list(characters), num_oov_indices=0, mask_token=None\n",
    ")\n",
    "\n",
    "# Mapping integers back to original characters\n",
    "num_to_char = layers.experimental.preprocessing.StringLookup(\n",
    "    vocabulary=char_to_num.get_vocabulary(), mask_token=None, invert=True\n",
    ")\n",
    "\n",
    "\n",
    "def split_data(images, labels, train_size=0.9, shuffle=True):\n",
    "    # 1. Get the total size of the dataset\n",
    "    size = len(images)\n",
    "    # 2. Make an indices array and shuffle it, if required\n",
    "    indices = np.arange(size)\n",
    "    if shuffle:\n",
    "        np.random.shuffle(indices)\n",
    "    # 3. Get the size of training samples\n",
    "    train_samples = int(size * train_size)\n",
    "    # 4. Split data into training and validation sets\n",
    "    x_train, y_train = images[indices[:train_samples]], labels[indices[:train_samples]]\n",
    "    x_valid, y_valid = images[indices[train_samples:]], labels[indices[train_samples:]]\n",
    "    return x_train, x_valid, y_train, y_valid\n",
    "\n",
    "\n",
    "# Splitting data into training and validation sets\n",
    "x_train, x_valid, y_train, y_valid = split_data(np.array(images), np.array(labels))\n",
    "\n",
    "\n",
    "def encode_single_sample(img_path, label):\n",
    "    # 1. Read image\n",
    "    img = tf.io.read_file(img_path)\n",
    "    # 2. Decode and convert to grayscale\n",
    "    img = tf.io.decode_png(img, channels=1)\n",
    "    # 3. Convert to float32 in [0, 1] range\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    # 4. Resize to the desired size\n",
    "    img = tf.image.resize(img, [img_height, img_width])\n",
    "    # 5. Transpose the image because we want the time\n",
    "    # dimension to correspond to the width of the image.\n",
    "    img = tf.transpose(img, perm=[1, 0, 2])\n",
    "    # 6. Map the characters in label to numbers\n",
    "    label = char_to_num(tf.strings.unicode_split(label, input_encoding=\"UTF-8\"))\n",
    "    # 7. Return a dict as our model is expecting two inputs\n",
    "    return {\"image\": img, \"label\": label}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataset objects\n",
    "split into train and valid dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_dataset = (\n",
    "    train_dataset.map(\n",
    "        encode_single_sample, num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
    "    )\n",
    "    .batch(batch_size)\n",
    "    .prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    ")\n",
    "\n",
    "validation_dataset = tf.data.Dataset.from_tensor_slices((x_valid, y_valid))\n",
    "validation_dataset = (\n",
    "    validation_dataset.map(\n",
    "        encode_single_sample, num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
    "    )\n",
    "    .batch(batch_size)\n",
    "    .prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the Data\n",
    "using plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "slice index 1 of dimension 0 out of bounds. [Op:StridedSlice] name: strided_slice/",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-efc94b5421ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"uint8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_join\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_to_char\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0max\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gray\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36m_slice_helper\u001b[0;34m(tensor, slice_spec, var)\u001b[0m\n\u001b[1;32m   1011\u001b[0m       \u001b[0mvar_empty\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1012\u001b[0m       \u001b[0mpacked_begin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpacked_end\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpacked_strides\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvar_empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1013\u001b[0;31m     return strided_slice(\n\u001b[0m\u001b[1;32m   1014\u001b[0m         \u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1015\u001b[0m         \u001b[0mpacked_begin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mstrided_slice\u001b[0;34m(input_, begin, end, strides, begin_mask, end_mask, ellipsis_mask, new_axis_mask, shrink_axis_mask, var, name)\u001b[0m\n\u001b[1;32m   1184\u001b[0m     \u001b[0mstrides\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1186\u001b[0;31m   op = gen_array_ops.strided_slice(\n\u001b[0m\u001b[1;32m   1187\u001b[0m       \u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m       \u001b[0mbegin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mstrided_slice\u001b[0;34m(input, begin, end, strides, begin_mask, end_mask, ellipsis_mask, new_axis_mask, shrink_axis_mask, name)\u001b[0m\n\u001b[1;32m  10318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10319\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m> 10320\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m  10321\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10322\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6841\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6842\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6843\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6844\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: slice index 1 of dimension 0 out of bounds. [Op:StridedSlice] name: strided_slice/"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 16 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "_, ax = plt.subplots(4, 4, figsize=(10, 5))\n",
    "for batch in train_dataset.take(1):\n",
    "    images = batch[\"image\"]\n",
    "    labels = batch[\"label\"]\n",
    "    for i in range(16):\n",
    "        img = (images[i] * 255).numpy().astype(\"uint8\")\n",
    "        label = tf.strings.reduce_join(num_to_char(labels[i])).numpy().decode(\"utf-8\")\n",
    "        ax[i // 4, i % 4].imshow(img[:, :, 0].T, cmap=\"gray\")\n",
    "        ax[i // 4, i % 4].set_title(label)\n",
    "        ax[i // 4, i % 4].axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "using class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CTCLayer(layers.Layer):\n",
    "    def __init__(self, name=None):\n",
    "        super().__init__(name=name)\n",
    "        self.loss_fn = keras.backend.ctc_batch_cost\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        # Compute the training-time loss value and add it\n",
    "        # to the layer using `self.add_loss()`.\n",
    "        batch_len = tf.cast(tf.shape(y_true)[0], dtype=\"int64\")\n",
    "        input_length = tf.cast(tf.shape(y_pred)[1], dtype=\"int64\")\n",
    "        label_length = tf.cast(tf.shape(y_true)[1], dtype=\"int64\")\n",
    "\n",
    "        input_length = input_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
    "        label_length = label_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
    "\n",
    "        loss = self.loss_fn(y_true, y_pred, input_length, label_length)\n",
    "        self.add_loss(loss)\n",
    "\n",
    "        # At test time, just return the computed predictions\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "def build_model():\n",
    "    # Inputs to the model\n",
    "    input_img = layers.Input(\n",
    "        shape=(img_width, img_height, 1), name=\"image\", dtype=\"float32\"\n",
    "    )\n",
    "    labels = layers.Input(name=\"label\", shape=(None,), dtype=\"float32\")\n",
    "\n",
    "    # First conv block\n",
    "    x = layers.Conv2D(\n",
    "        32,\n",
    "        (3, 3),\n",
    "        activation=\"relu\",\n",
    "        kernel_initializer=\"he_normal\",\n",
    "        padding=\"same\",\n",
    "        name=\"Conv1\",\n",
    "    )(input_img)\n",
    "    x = layers.MaxPooling2D((2, 2), name=\"pool1\")(x)\n",
    "\n",
    "    # Second conv block\n",
    "    x = layers.Conv2D(\n",
    "        64,\n",
    "        (3, 3),\n",
    "        activation=\"relu\",\n",
    "        kernel_initializer=\"he_normal\",\n",
    "        padding=\"same\",\n",
    "        name=\"Conv2\",\n",
    "    )(x)\n",
    "    x = layers.MaxPooling2D((2, 2), name=\"pool2\")(x)\n",
    "\n",
    "    # We have used two max pool with pool size and strides 2.\n",
    "    # Hence, downsampled feature maps are 4x smaller. The number of\n",
    "    # filters in the last layer is 64. Reshape accordingly before\n",
    "    # passing the output to the RNN part of the model\n",
    "    new_shape = ((img_width // 4), (img_height // 4) * 64)\n",
    "    x = layers.Reshape(target_shape=new_shape, name=\"reshape\")(x)\n",
    "    x = layers.Dense(64, activation=\"relu\", name=\"dense1\")(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "\n",
    "    # RNNs\n",
    "    x = layers.Bidirectional(layers.LSTM(128, return_sequences=True, dropout=0.25))(x)\n",
    "    x = layers.Bidirectional(layers.LSTM(64, return_sequences=True, dropout=0.25))(x)\n",
    "\n",
    "    # Output layer\n",
    "    x = layers.Dense(len(characters) + 1, activation=\"softmax\", name=\"dense2\")(x)\n",
    "\n",
    "    # Add CTC layer for calculating CTC loss at each step\n",
    "    output = CTCLayer(name=\"ctc_loss\")(labels, x)\n",
    "\n",
    "    # Define the model\n",
    "    model = keras.models.Model(\n",
    "        inputs=[input_img, labels], outputs=output, name=\"ocr_model_v1\"\n",
    "    )\n",
    "    # Optimizer\n",
    "    opt = keras.optimizers.Adam()\n",
    "    # Compile the model and return\n",
    "    model.compile(optimizer=opt)\n",
    "    return model\n",
    "\n",
    "\n",
    "# Get the model\n",
    "model = build_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "early_stopping_patience = 10\n",
    "# Add early stopping\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\", patience=early_stopping_patience, restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=validation_dataset,\n",
    "    epochs=epochs,\n",
    "    callbacks=[early_stopping],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the prediction model by extracting layers till the output layer\n",
    "prediction_model = keras.models.Model(\n",
    "    model.get_layer(name=\"image\").input, model.get_layer(name=\"dense2\").output\n",
    ")\n",
    "prediction_model.summary()\n",
    "\n",
    "# A utility function to decode the output of the network\n",
    "def decode_batch_predictions(pred):\n",
    "    input_len = np.ones(pred.shape[0]) * pred.shape[1]\n",
    "    # Use greedy search. For complex tasks, you can use beam search\n",
    "    results = keras.backend.ctc_decode(pred, input_length=input_len, greedy=True)[0][0][\n",
    "        :, :max_length\n",
    "    ]\n",
    "    # Iterate over the results and get back the text\n",
    "    output_text = []\n",
    "    for res in results:\n",
    "        res = tf.strings.reduce_join(num_to_char(res)).numpy().decode(\"utf-8\")\n",
    "        output_text.append(res)\n",
    "    return output_text\n",
    "\n",
    "\n",
    "#  Let's check results on some validation samples\n",
    "for batch in validation_dataset.take(1):\n",
    "    batch_images = batch[\"image\"]\n",
    "    batch_labels = batch[\"label\"]\n",
    "\n",
    "    preds = prediction_model.predict(batch_images)\n",
    "    pred_texts = decode_batch_predictions(preds)\n",
    "\n",
    "    orig_texts = []\n",
    "    for label in batch_labels:\n",
    "        label = tf.strings.reduce_join(num_to_char(label)).numpy().decode(\"utf-8\")\n",
    "        orig_texts.append(label)\n",
    "\n",
    "    _, ax = plt.subplots(4, 4, figsize=(15, 5))\n",
    "    for i in range(len(pred_texts)):\n",
    "        img = (batch_images[i, :, :, 0] * 255).numpy().astype(np.uint8)\n",
    "        img = img.T\n",
    "        title = f\"Prediction: {pred_texts[i]}\"\n",
    "        ax[i // 4, i % 4].imshow(img, cmap=\"gray\")\n",
    "        ax[i // 4, i % 4].set_title(title)\n",
    "        ax[i // 4, i % 4].axis(\"off\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
